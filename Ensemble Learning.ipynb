{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f36a46f",
   "metadata": {},
   "source": [
    "# *Ensemble Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb0da7",
   "metadata": {},
   "source": [
    "## 1. What is Ensemble Learning in machine learning? Explain the key idea behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c130fca",
   "metadata": {},
   "source": [
    "Ensemble Learning combines multiple models (called base learners) to get better accuracy and stability than a single model.\n",
    "The key idea: “Many weak models together can form a strong model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b0ac2",
   "metadata": {},
   "source": [
    "## 2. What is the difference between Bagging and Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e3d2a",
   "metadata": {},
   "source": [
    "| Feature  | Bagging                         | Boosting                                  |\n",
    "| -------- | ------------------------------- | ----------------------------------------- |\n",
    "| Goal     | Reduce variance                 | Reduce bias                               |\n",
    "| Training | Models trained independently    | Models trained sequentially               |\n",
    "| Sample   | Random samples with replacement | Each new model focuses on previous errors |\n",
    "| Example  | Random Forest                   | AdaBoost, XGBoost                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8f130c",
   "metadata": {},
   "source": [
    "## 3. What is bootstrap sampling and what role does it play in Bagging methods like Random Forest? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470c2b90",
   "metadata": {},
   "source": [
    "- **Bootstrap sampling** means randomly selecting samples with replacement from the dataset to create multiple subsets.\n",
    "- In **Bagging** (like Random Forest), each tree is trained on a different bootstrap sample. This increases diversity and reduces overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d43429",
   "metadata": {},
   "source": [
    "## 4. What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5271ec",
   "metadata": {},
   "source": [
    "OOB samples are the data points not selected in a bootstrap sample.\n",
    "The model uses these unseen samples to test performance — giving an OOB score, which acts like built-in cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f300fe6",
   "metadata": {},
   "source": [
    "## 5. Compare feature importance analysis in a single Decision Tree vs. a Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12537958",
   "metadata": {},
   "source": [
    "- A single Decision Tree gives feature importance based on how much each feature reduces impurity.\n",
    "\n",
    "- A Random Forest averages these importance scores across many trees, making the ranking more stable and reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429b54fd",
   "metadata": {},
   "source": [
    "## 6. Write a Python program to: Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer() \n",
    "● Train a Random Forest Classifier \n",
    "● Print the top 5 most important features based on feature importance scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "163fa791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst area: 0.1394\n",
      "worst concave points: 0.1322\n",
      "mean concave points: 0.1070\n",
      "worst radius: 0.0828\n",
      "worst perimeter: 0.0808\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Train model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "indices = importances.argsort()[::-1][:5]\n",
    "\n",
    "for i in indices:\n",
    "    print(f\"{data.feature_names[i]}: {importances[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef6cc1",
   "metadata": {},
   "source": [
    "## 7. Write a Python program to: \n",
    "- **Train a Bagging Classifier using Decision Trees on the Iris dataset** \n",
    "- **Evaluate its accuracy and compare with a single Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecdbbc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 1.0\n",
      "Bagging Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Data split\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Single Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "acc_dt = accuracy_score(y_test, dt.predict(X_test))\n",
    "\n",
    "# Bagging with Decision Trees\n",
    "bag = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
    "bag.fit(X_train, y_train)\n",
    "acc_bag = accuracy_score(y_test, bag.predict(X_test))\n",
    "\n",
    "print(\"Decision Tree Accuracy:\", acc_dt)\n",
    "print(\"Bagging Accuracy:\", acc_bag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d66a852",
   "metadata": {},
   "source": [
    "## 8. Write a Python program to: \n",
    "- Train a Random Forest Classifier \n",
    "- Tune hyperparameters max_depth and n_estimators using GridSearchCV \n",
    "- Print the best parameters and final accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ca61734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 7, 'n_estimators': 50}\n",
      "Final Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "param_grid = {'max_depth': [3, 5, 7, None], 'n_estimators': [50, 100, 150]}\n",
    "grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "acc = accuracy_score(y_test, best_model.predict(X_test))\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Final Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5020698c",
   "metadata": {},
   "source": [
    "## 9. Write a Python program to: \n",
    "- Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset \n",
    "- Compare their Mean Squared Errors (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34694138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging MSE: 0.2582477439355284\n",
      "Random Forest MSE: 0.2576688724828818\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "bag = BaggingRegressor(DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
    "rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "mse_bag = mean_squared_error(y_test, bag.predict(X_test))\n",
    "mse_rf = mean_squared_error(y_test, rf.predict(X_test))\n",
    "\n",
    "print(\"Bagging MSE:\", mse_bag)\n",
    "print(\"Random Forest MSE:\", mse_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc7bd15",
   "metadata": {},
   "source": [
    "## 10. You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. \n",
    "\n",
    "You decide to use ensemble techniques to increase model performance. \n",
    "\n",
    "Explain your step-by-step approach to: \n",
    "- Choose between Bagging or Boosting \n",
    "- Handle overfitting \n",
    "- Select base models \n",
    "- Evaluate performance using cross-validation \n",
    "- Justify how ensemble learning improves decision-making in this real-world \n",
    "context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f25fce3",
   "metadata": {},
   "source": [
    "1. Choose between Bagging or Boosting:\n",
    "    - If data is noisy → use Bagging (Random Forest) to reduce variance.\n",
    "    - If model underfits → use Boosting (XGBoost, AdaBoost) to reduce bias.\n",
    "\n",
    "2. Handle Overfitting:\n",
    "    - Use fewer tree depths, limit estimators, apply regularization, and cross-validation.\n",
    "\n",
    "3. Select Base Models:\n",
    "    - Decision Trees (for interpretability) or Logistic Regression (for stability).\n",
    "\n",
    "4. Evaluate Performance:\n",
    "    - Use k-Fold Cross-Validation, check metrics like AUC, F1-score, Precision-Recall.\n",
    "\n",
    "5. Justification:\n",
    "    - Ensemble models combine multiple weak learners, improving accuracy and reducing risk of misclassifying potential defaulters.\n",
    "    - This helps financial institutions make more reliable lending decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
