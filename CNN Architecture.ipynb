{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c161bb52",
   "metadata": {},
   "source": [
    "# CNN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72db321c",
   "metadata": {},
   "source": [
    "## 1. What is the role of filters and feature maps in Convolutional Neural Network (CNN)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c71d034",
   "metadata": {},
   "source": [
    "### Role of Filters and Feature Maps in CNN\n",
    "\n",
    "In a Convolutional Neural Network (CNN), **filters** and **feature maps** play a very important role in extracting useful information from images.\n",
    "\n",
    "#### Filters (Kernels)\n",
    "- Filters are small matrices (for example: 3×3 or 5×5).\n",
    "- They slide over the input image during convolution.\n",
    "- Each filter detects a specific feature such as:\n",
    "  - edges\n",
    "  - corners\n",
    "  - textures\n",
    "  - shapes\n",
    "- The values inside the filter are learned automatically during training.\n",
    "\n",
    "#### Feature Maps\n",
    "- A feature map is the output produced after applying a filter to the input image.\n",
    "- Each filter creates **one feature map**.\n",
    "- Feature maps highlight where a specific feature is present in the image.\n",
    "- Deeper layers produce feature maps with more complex features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbfbe71",
   "metadata": {},
   "source": [
    "## 2. Explain the concepts of padding and stride in CNNs. How do they affect the output dimensions of feature maps?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbceb13",
   "metadata": {},
   "source": [
    "### Padding and Stride in Convolutional Neural Networks (CNN)\n",
    "\n",
    "#### Padding\n",
    "- Padding means adding extra pixels (usually zeros) around the border of the input image.\n",
    "- It is used to control the size of the output feature map.\n",
    "- Padding helps in:\n",
    "  - preserving edge information\n",
    "  - preventing the image from shrinking too quickly\n",
    "\n",
    "**Types of Padding**\n",
    "- **Valid padding**: No padding is added → output size reduces.\n",
    "- **Same padding**: Padding is added so output size remains same as input size.\n",
    "\n",
    "#### Stride\n",
    "- Stride defines how many pixels the filter moves at a time.\n",
    "- A stride of 1 means the filter moves one pixel at a time.\n",
    "- A larger stride skips pixels and reduces the output size.\n",
    "\n",
    "#### Effect on Output Size\n",
    "- Increasing **padding** → increases or preserves output size.\n",
    "- Increasing **stride** → decreases output size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817efe44",
   "metadata": {},
   "source": [
    "## 3. Define receptive field in the context of CNNs. Why is it important for deep architectures?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9708f1a",
   "metadata": {},
   "source": [
    "### Receptive Field in Convolutional Neural Networks (CNN)\n",
    "\n",
    "#### What is Receptive Field?\n",
    "- The receptive field is the **region of the input image** that a neuron (or feature) in a CNN layer can \"see\".\n",
    "- It tells how much of the original image influences a single output value.\n",
    "- In simple words, it is the **area of the input image used to make a decision**.\n",
    "\n",
    "#### How Receptive Field Grows\n",
    "- In early layers, the receptive field is small and captures simple features like edges.\n",
    "- As we go deeper in the network:\n",
    "  - more convolution layers\n",
    "  - pooling layers\n",
    "- the receptive field increases and captures larger patterns like objects and shapes.\n",
    "\n",
    "#### Importance in Deep Architectures\n",
    "- A larger receptive field helps the model understand:\n",
    "  - global context\n",
    "  - spatial relationships\n",
    "- It allows deep CNNs to recognize complex objects instead of just small features.\n",
    "- Without a growing receptive field, the network cannot learn high-level features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3262986d",
   "metadata": {},
   "source": [
    "## 4. Discuss how filter size and stride influence the number of parameters in a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa6bd22",
   "metadata": {},
   "source": [
    "### Effect of Filter Size and Stride on Number of Parameters in CNN\n",
    "\n",
    "#### Filter Size\n",
    "- Filter size decides how many values (weights) are inside a filter.\n",
    "- Larger filters contain more parameters.\n",
    "- Number of parameters in one filter is calculated as:\n",
    "\n",
    "Parameters = (Filter Height × Filter Width × Input Channels) + Bias\n",
    "\n",
    "#### Example\n",
    "- 3×3 filter with 1 input channel:\n",
    "  - Parameters = (3×3×1) + 1 = 10\n",
    "- 5×5 filter with 1 input channel:\n",
    "  - Parameters = (5×5×1) + 1 = 26\n",
    "- Larger filters increase:\n",
    "  - model size\n",
    "  - computation cost\n",
    "  - risk of overfitting\n",
    "\n",
    "#### Stride\n",
    "- Stride does **not directly change** the number of parameters.\n",
    "- It affects how many times the filter is applied on the input.\n",
    "- Larger stride:\n",
    "  - reduces output feature map size\n",
    "  - reduces computation\n",
    "- Smaller stride:\n",
    "  - keeps more spatial information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0f49d3",
   "metadata": {},
   "source": [
    "## Compare and contrast CNN architectures: LeNet, AlexNet, and VGG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20c75e",
   "metadata": {},
   "source": [
    "### Comparison of LeNet, AlexNet, and VGG CNN Architectures\n",
    "\n",
    "#### LeNet\n",
    "- One of the earliest CNN architectures.\n",
    "- Designed mainly for handwritten digit recognition (MNIST).\n",
    "- Shallow network with fewer layers.\n",
    "- Uses small input images (28×28).\n",
    "- Performance is good for simple tasks but not suitable for complex images.\n",
    "\n",
    "\n",
    "#### AlexNet\n",
    "- First deep CNN that showed huge success in image classification.\n",
    "- Won the ImageNet competition in 2012.\n",
    "- Introduced GPU training and ReLU activation.\n",
    "- Handles large and complex images.\n",
    "\n",
    "\n",
    "#### VGG\n",
    "- Very deep CNN architecture.\n",
    "- Uses only small filters (3×3) but many layers.\n",
    "- Focuses on depth to improve performance.\n",
    "- High accuracy but very high computational cost.\n",
    "\n",
    "\n",
    "#### Comparison Table\n",
    "\n",
    "| Feature      | LeNet        | AlexNet        | VGG           |\n",
    "|-------------|--------------|----------------|---------------|\n",
    "| Depth       | Shallow      | Medium         | Very Deep     |\n",
    "| Filter Size | Large        | Large → Medium | Small (3×3)   |\n",
    "| Complexity  | Low          | Medium         | High          |\n",
    "| Performance | Basic        | Good           | Excellent     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b9a40e",
   "metadata": {},
   "source": [
    "## 6. Build and train a simple CNN model on the MNIST dataset using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d6b7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n",
      "c:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.9474 - loss: 0.1735 - val_accuracy: 0.9837 - val_loss: 0.0607\n",
      "Epoch 2/5\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 22ms/step - accuracy: 0.9844 - loss: 0.0503 - val_accuracy: 0.9830 - val_loss: 0.0551\n",
      "Epoch 3/5\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 23ms/step - accuracy: 0.9888 - loss: 0.0356 - val_accuracy: 0.9902 - val_loss: 0.0416\n",
      "Epoch 4/5\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 20ms/step - accuracy: 0.9916 - loss: 0.0259 - val_accuracy: 0.9910 - val_loss: 0.0384\n",
      "Epoch 5/5\n",
      "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9936 - loss: 0.0198 - val_accuracy: 0.9887 - val_loss: 0.0349\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9887 - loss: 0.0331\n",
      "Test Accuracy: 0.9886999726295471\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape data to include channel dimension\n",
    "X_train = X_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "X_test = X_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "\n",
    "# One-hot encode target labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78580e4e",
   "metadata": {},
   "source": [
    "## 7. Load and preprocess the CIFAR-10 dataset using Keras, and create a CNN model to classify RGB images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a8f803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 0us/step\n",
      "Epoch 1/5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 26ms/step - accuracy: 0.3816 - loss: 1.6859 - val_accuracy: 0.5228 - val_loss: 1.3448\n",
      "Epoch 2/5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.5183 - loss: 1.3510 - val_accuracy: 0.5886 - val_loss: 1.1662\n",
      "Epoch 3/5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 27ms/step - accuracy: 0.5716 - loss: 1.2103 - val_accuracy: 0.6204 - val_loss: 1.0847\n",
      "Epoch 4/5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 32ms/step - accuracy: 0.6050 - loss: 1.1247 - val_accuracy: 0.6558 - val_loss: 1.0031\n",
      "Epoch 5/5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 32ms/step - accuracy: 0.6262 - loss: 1.0633 - val_accuracy: 0.6666 - val_loss: 0.9426\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6586 - loss: 0.9727\n",
      "Test Accuracy: 0.6585999727249146\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb27f5",
   "metadata": {},
   "source": [
    "## 8. Define and train a CNN on the MNIST dataset using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8235e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb3b19d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:03<00:00, 3.04MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 122kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.03MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.27MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1580375699147181\n",
      "Epoch 2, Loss: 0.04634912982411774\n",
      "Epoch 3, Loss: 0.03244273949310612\n",
      "Epoch 4, Loss: 0.023735606959258426\n",
      "Epoch 5, Loss: 0.01794759144512331\n",
      "Test Accuracy: 0.9886\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(\"Test Accuracy:\", correct / total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c80037",
   "metadata": {},
   "source": [
    "## 9. Train a CNN using Keras ImageDataGenerator on a custom image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8692ae07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.5000 - loss: 0.6588\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 573ms/step - accuracy: 0.5000 - loss: 4.6685\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 495ms/step - accuracy: 0.5000 - loss: 0.6883\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 611ms/step - accuracy: 1.0000 - loss: 1.0663e-05\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490ms/step - accuracy: 1.0000 - loss: 2.1019e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1cac1597b00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Image parameters\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batch_size = 32\n",
    "\n",
    "# ImageDataGenerator (NO validation split)\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "# Load image data from directory\n",
    "train_data = datagen.flow_from_directory(\n",
    "    'dataset/',                # Path to dataset folder\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_data,\n",
    "    epochs=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3be6ad",
   "metadata": {},
   "source": [
    "## 10.End-to-end approach to build and deploy a CNN model for classifying Chest X-ray images (Normal vs Pneumonia) using Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac565e",
   "metadata": {},
   "source": [
    "### Step 1: Data Preparation\n",
    "- Collect chest X-ray images and organize them into folders:\n",
    "  - dataset/\n",
    "    - train/\n",
    "      - Normal/\n",
    "      - Pneumonia/\n",
    "    - test/\n",
    "      - Normal/\n",
    "      - Pneumonia/\n",
    "- Resize images to a fixed size (e.g., 224×224).\n",
    "- Normalize pixel values to range [0,1].\n",
    "- Use data augmentation to avoid overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfd4586a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing using ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    'dataset/',\n",
    "    target_size=(224,224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "    'dataset/',\n",
    "    target_size=(224,224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa641478",
   "metadata": {},
   "source": [
    "### Step 2: Model Building and Training\n",
    "- Use a CNN with convolution, pooling, and dense layers.\n",
    "- Binary classification using sigmoid activation.\n",
    "- Binary cross-entropy loss is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df136851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.5000 - loss: 0.6909WARNING:tensorflow:5 out of the last 3532 calls to <function TensorFlowTrainer._make_function.<locals>.multi_step_on_iterator at 0x000001CAC2A1FB00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.5000 - loss: 0.6909 - val_accuracy: 1.0000 - val_loss: 0.3441\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.3901 - val_accuracy: 1.0000 - val_loss: 2.1040e-05\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 3.6588e-05 - val_accuracy: 0.5000 - val_loss: 0.4800\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0129 - val_accuracy: 1.0000 - val_loss: 0.1248\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 681ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 9.5989e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# CNN model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3)),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(train_data, epochs=5, validation_data=test_data)\n",
    "\n",
    "# Save model\n",
    "model.save(\"xray_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed280ad",
   "metadata": {},
   "source": [
    "### Step 3: Model Deployment using Streamlit\n",
    "- Load the trained model.\n",
    "- Accept image upload from user.\n",
    "- Preprocess image and predict result.\n",
    "- Display prediction on web interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d144ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-12-24 12:15:46.262 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "# streamlit_app.py\n",
    "import streamlit as st\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "model = tf.keras.models.load_model(\"xray_model.h5\")\n",
    "\n",
    "st.title(\"Chest X-ray Classification\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Upload X-ray Image\", type=[\"jpg\",\"png\"])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    image = Image.open(uploaded_file).resize((224,224))\n",
    "    img_array = np.array(image) / 255.0\n",
    "    img_array = img_array.reshape(1,224,224,3)\n",
    "\n",
    "    prediction = model.predict(img_array)\n",
    "\n",
    "    if prediction[0][0] > 0.5:\n",
    "        st.write(\"Prediction: Pneumonia\")\n",
    "    else:\n",
    "        st.write(\"Prediction: Normal\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acde63c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1a1e3e3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
