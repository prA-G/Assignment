{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a18625b4",
   "metadata": {},
   "source": [
    "# *DECISION TREE ASSIGNMENT*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f730c370",
   "metadata": {},
   "source": [
    "## What is a Decision Tree, and how does it work in the context of classification? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1f570f",
   "metadata": {},
   "source": [
    "A **Decision Tree** is a type of supervised learning algorithm that is used for both **classification** and **regression** problems.  \n",
    "In simple words, it works like a flowchart — we start from the top (root node), ask questions at each step (internal nodes), and based on the answers (yes/no or true/false), we move down different branches until we reach a final decision (leaf node).\n",
    "\n",
    "For **classification**, the goal is to split the data into groups that are as pure as possible — meaning each group mostly belongs to one class.\n",
    "\n",
    "Here’s how it works step-by-step:\n",
    "1. The algorithm picks the **best feature** to split the data using measures like **Gini Impurity** or **Information Gain (Entropy)**.\n",
    "2. It divides the dataset based on that feature.\n",
    "3. This process keeps repeating for each branch until:\n",
    "   - All the data in a node belongs to one class, or  \n",
    "   - No further splits can improve the classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463d5d9e",
   "metadata": {},
   "source": [
    "## 2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa0c93",
   "metadata": {},
   "source": [
    "When a Decision Tree splits data, it tries to make each group (node) as \"pure\" as possible — meaning most samples in that node belong to the same class.  \n",
    "To measure how pure or impure a node is, we use **impurity measures** like **Gini Impurity** and **Entropy**.\n",
    "####  Gini Impurity:\n",
    "Gini measures how often a randomly chosen element would be **incorrectly classified** if we randomly label it based on class distribution.\n",
    "####  Entropy:\n",
    "Entropy measures the **uncertainty** or **randomness** in the data.\n",
    "#### Impact on Splits:\n",
    "Both Gini and Entropy help the Decision Tree choose **where to split**:\n",
    "- The algorithm tests different features and finds the one that gives the **lowest impurity after the split**.\n",
    "- The **cleaner** (purer) the resulting groups, the **better** the split.\n",
    "\n",
    "So basically:\n",
    "- **Gini** is faster to compute and often used by default (like in `sklearn`).\n",
    "- **Entropy** gives a more detailed measure of disorder but behaves quite similarly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d6bd03",
   "metadata": {},
   "source": [
    "## 3.  What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca02136",
   "metadata": {},
   "source": [
    "When a Decision Tree grows, it can easily become too complex — learning even the noise in the data (overfitting).  \n",
    "**Pruning** is the technique used to simplify the tree and improve its ability to generalize.\n",
    "\n",
    "---\n",
    "\n",
    "####  Pre-Pruning (Early Stopping):\n",
    "In **pre-pruning**, we **stop the tree from growing too deep** during the building process itself.\n",
    "\n",
    "This is done by setting some limits like:\n",
    "- Maximum depth of the tree (`max_depth`)\n",
    "- Minimum samples required to split a node (`min_samples_split`)\n",
    "- Minimum samples required at a leaf node (`min_samples_leaf`)\n",
    "**Advantage:**  \n",
    "- Prevents the tree from becoming too large and overfitting early on, saving both **time** and **computational cost**.\n",
    "\n",
    "####  Post-Pruning (Reduced Error Pruning):\n",
    "In **post-pruning**, the tree is **fully grown first**, and then we **trim unnecessary branches** that don’t improve performance on a validation set.\n",
    "\n",
    "Steps:\n",
    "1. Grow the full tree.\n",
    "2. Evaluate performance on validation data.\n",
    "3. Remove branches that don’t add much predictive power.\n",
    "\n",
    "**Advantage:**  \n",
    "- Helps improve **model accuracy and generalization** by removing weak or noisy splits after seeing the whole picture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4011c2",
   "metadata": {},
   "source": [
    "## 4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0196d3b7",
   "metadata": {},
   "source": [
    "**Information Gain (IG)** is a measure used in Decision Trees to decide **which feature to split on** at each step.  \n",
    "It tells us how much **“information” or “purity” improvement** we get when we split the dataset using a particular feature.\n",
    "\n",
    "####  Why It’s Important:\n",
    "Information Gain helps the Decision Tree **choose the best attribute** to split on at each level.  \n",
    "The algorithm always picks the feature with the **highest Information Gain**, because it leads to the **most significant reduction in uncertainty**.\n",
    "\n",
    "\n",
    "**Example:**  \n",
    "If we’re classifying whether someone will buy a phone:\n",
    "- Splitting by “Income” might give higher IG than “Age”  \n",
    "- So the tree will choose **Income** as the first split feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8882a7e0",
   "metadata": {},
   "source": [
    "## 5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b300a70",
   "metadata": {},
   "source": [
    "####  Real-World Applications:\n",
    "1. **Finance:**  \n",
    "   Used for credit scoring, loan approval, and detecting fraud.  \n",
    "   Example: Predicting whether a person is likely to repay a loan or not.\n",
    "\n",
    "2. **Healthcare:**  \n",
    "   Helps in diagnosing diseases based on symptoms and test results.  \n",
    "   Example: Predicting if a tumor is malignant or benign.\n",
    "\n",
    "3. **Marketing:**  \n",
    "   Used for customer segmentation and predicting buying behavior.  \n",
    "   Example: Will a customer buy a product after seeing an ad?\n",
    "\n",
    "4. **HR and Hiring:**  \n",
    "   Helps filter job applicants based on their qualifications and experience.\n",
    "\n",
    "5. **Manufacturing:**  \n",
    "   Used for quality control and predicting machine failures.\n",
    "\n",
    "\n",
    "####  Advantages:\n",
    "1. **Easy to Understand and Interpret:**  \n",
    "   The tree structure looks like a simple flowchart — great for explaining decisions to non-technical people.\n",
    "\n",
    "2. **Handles Both Numerical and Categorical Data:**  \n",
    "   Works well even when data types are mixed.\n",
    "\n",
    "3. **No Need for Feature Scaling:**  \n",
    "   Unlike algorithms like SVM or KNN, decision trees don’t need normalization or standardization.\n",
    "\n",
    "4. **Good for Small to Medium Datasets:**  \n",
    "   They can perform well without needing huge amounts of data.\n",
    "\n",
    "\n",
    "#### Limitations:\n",
    "1. **Overfitting:**  \n",
    "   If not pruned properly, trees can become too deep and learn noise instead of actual patterns.\n",
    "\n",
    "2. **Unstable:**  \n",
    "   Small changes in data can create a completely different tree.\n",
    "\n",
    "3. **Biased Toward Features with More Levels:**  \n",
    "   Features with many categories might dominate splits.\n",
    "\n",
    "4. **Not Great for Continuous Predictions Alone:**  \n",
    "   Decision Trees can give step-like predictions, which may not be smooth for regression tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54e524e",
   "metadata": {},
   "source": [
    "## 6. Write a Python program to: \n",
    "● Load the Iris Dataset \n",
    "● Train a Decision Tree Classifier using the Gini criterion \n",
    "● Print the model’s accuracy and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64c0706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 100.0 %\n",
      "\n",
      "Feature Importances:\n",
      "sepal length (cm): 0.0\n",
      "sepal width (cm): 0.017\n",
      "petal length (cm): 0.906\n",
      "petal width (cm): 0.077\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data        # features\n",
    "y = iris.target      # labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", round(accuracy * 100, 2), \"%\")\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "for name, importance in zip(iris.feature_names, model.feature_importances_):\n",
    "    print(f\"{name}: {round(importance, 3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef6de2a",
   "metadata": {},
   "source": [
    "## 7. Question 7:  Write a Python program to: \n",
    "● Load the Iris Dataset \n",
    "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to \n",
    "a fully-grown tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b062d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (max_depth=3): 100.0 %\n",
      "Accuracy (fully-grown tree): 100.0 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tree_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_limited.fit(X_train, y_train)\n",
    "\n",
    "tree_full = DecisionTreeClassifier(random_state=42)\n",
    "tree_full.fit(X_train, y_train)\n",
    "\n",
    "y_pred_limited = tree_limited.predict(X_test)\n",
    "y_pred_full = tree_full.predict(X_test)\n",
    "\n",
    "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
    "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
    "\n",
    "print(\"Accuracy (max_depth=3):\", round(accuracy_limited * 100, 2), \"%\")\n",
    "print(\"Accuracy (fully-grown tree):\", round(accuracy_full * 100, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2391d134",
   "metadata": {},
   "source": [
    "## 8. Question 8: Write a Python program to: \n",
    "● Load the Boston Housing Dataset \n",
    "● Train a Decision Tree Regressor \n",
    "● Print the Mean Squared Error (MSE) and feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9832312b",
   "metadata": {},
   "source": [
    "Since load_boston got removed from scikit-learn for ethical reasons, so in recent versions you can’t use it directly So I am using fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e9a3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.495\n",
      "\n",
      "Feature Importances:\n",
      "MedInc: 0.529\n",
      "HouseAge: 0.052\n",
      "AveRooms: 0.053\n",
      "AveBedrms: 0.029\n",
      "Population: 0.031\n",
      "AveOccup: 0.131\n",
      "Latitude: 0.094\n",
      "Longitude: 0.083\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", round(mse, 3))\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "for name, importance in zip(data.feature_names, regressor.feature_importances_):\n",
    "    print(f\"{name}: {round(importance, 3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53592efa",
   "metadata": {},
   "source": [
    "## 9. Question 9: Write a Python program to: \n",
    "● Load the Iris Dataset \n",
    "● Tune the Decision Tree’s max_depth and min_samples_split using \n",
    "GridSearchCV \n",
    "● Print the best parameters and the resulting model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c581afae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
      "Accuracy with Best Parameters: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "dtree = DecisionTreeClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, None],\n",
    "    'min_samples_split': [2, 5, 10, 15]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=dtree, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy with Best Parameters:\", round(accuracy * 100, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e09c0a",
   "metadata": {},
   "source": [
    "## 10. Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. \n",
    "Explain the step-by-step process you would follow to: \n",
    "● Handle the missing values \n",
    "● Encode the categorical features \n",
    "● Train a Decision Tree model \n",
    "● Tune its hyperparameters \n",
    "● Evaluate its performance And describe what business value this model could provide in the real-world setting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b960d",
   "metadata": {},
   "source": [
    "If I were working on a healthcare dataset with mixed data types and missing values, here’s how I’d handle it:\n",
    "\n",
    "#### Handle Missing Values:\n",
    "- **Identify missing data** in both numeric and categorical features.  \n",
    "- **Numeric columns:** Impute missing values with **mean** or **median**. Median is better if there are outliers.  \n",
    "- **Categorical columns:** Impute missing values with **mode** (most frequent category).  \n",
    "- Use libraries like `pandas` or `sklearn.impute.SimpleImputer` for this.\n",
    "\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "```\n",
    "\n",
    "#### Encode Categorical Features:\n",
    "\n",
    "- Convert categorical variables to numbers for the model to understand.\n",
    "- Decision Trees can handle ordinal numbers, but for non-ordinal categories, use One-Hot Encoding.\n",
    "- Use `pd.get_dummies()` or `sklearn.preprocessing.OneHotEncoder`.\n",
    "\n",
    "#### Train a Decision Tree Model:\n",
    "\n",
    "- Split the dataset into training and testing sets.\n",
    "- Initialize the Decision Tree (DecisionTreeClassifier).\n",
    "- Fit the model on the training data.\n",
    "\n",
    "```python \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "#### Tune Hyperparameters:\n",
    "\n",
    "- Use GridSearchCV or RandomizedSearchCV to find the best parameters like:\n",
    "    - `max_depth`\n",
    "    - `min_samples_split`\n",
    "    - `min_samples_leaf`\n",
    "    - `criterion` (Gini or Entropy)\n",
    "\n",
    "- This prevents overfitting and improves generalization.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [3,5,7,None],\n",
    "    'min_samples_split': [2,5,10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "```\n",
    "\n",
    "#### Evaluate Performance:\n",
    "\n",
    "- Check model accuracy on test data.\n",
    "- Also use metrics like:\n",
    "    - Precision & Recall (important for healthcare to avoid false negatives)\n",
    "    - F1-score\n",
    "    - Confusion matrix for detailed insight.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
